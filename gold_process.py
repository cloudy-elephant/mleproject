# -*- coding: utf-8 -*-
"""
gold_process.py

åŠŸèƒ½ï¼š
ä» silver/label_store ä¸­è¯»å–åˆ†æœˆçš„ label_xxx.csvï¼Œ
ä¸ silver/feature_store ä¸­çš„ 4 å¼ ç‰¹å¾è¡¨æŒ‰ customerID åˆå¹¶ï¼Œ
è¾“å‡ºåˆ° gold/feature_storeã€‚

è¾“å‡ºæ–‡ä»¶åï¼š gold_yyyy_mm_dd.csv
"""
import re
from pathlib import Path
from typing import Iterable, Tuple, Dict, Optional
import pandas as pd
import os

# === è·¯å¾„å®šä¹‰ ===
BASE_DIR = Path(r"C:\Users\HP\Desktop\MLE\mleproject")

LABEL_DIR = BASE_DIR / "datamart" / "silver" / "lable_store"
FEATURE_DIR = BASE_DIR / "datamart" / "silver" / "feature_store"
OUT_DIR = BASE_DIR / "datamart" / "gold"

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
OUT_DIR.mkdir(parents=True, exist_ok=True)

# å››å¼ ç‰¹å¾è¡¨è·¯å¾„
feature_paths = {
    "contract": FEATURE_DIR / "contract_df_clean.csv",
    "service": FEATURE_DIR / "service_df_clean.csv",
    "demographic": FEATURE_DIR / "demographic_df_clean.csv",
    "financial": FEATURE_DIR / "financial_df_clean.csv",
}

# === è¾…åŠ©å‡½æ•° ===
def log(msg):
    print(f"[gold] {msg}")

def safe_read_csv(path: Path):
    if not path.exists():
        log(f"âš ï¸ æœªæ‰¾åˆ°æ–‡ä»¶: {path.name}ï¼Œè·³è¿‡ã€‚")
        return pd.DataFrame()
    df = pd.read_csv(path)
    if "customerID" not in df.columns:
        log(f"âš ï¸ {path.name} ç¼ºå°‘ customerIDï¼Œè·³è¿‡ã€‚")
        return pd.DataFrame()
    df["customerID"] = df["customerID"].astype(str).str.strip()
    return df

def drop_all_duplicate_customer_ids_in_gold(base_dir):

    base = Path(base_dir)
    gold_dir = base / "datamart" / "gold" / "feature_store"
    if not gold_dir.exists():
        gold_dir = base / "datamart" / "gold"
    if not gold_dir.exists():
        raise FileNotFoundError(f"æœªæ‰¾åˆ° gold ç›®å½•ï¼š{gold_dir}")

    report = []
    files = sorted(gold_dir.glob("gold_*.csv"))
    if not files:
        raise FileNotFoundError(f"ç›®å½•ä¸­æœªæ‰¾åˆ° gold_*.csvï¼š{gold_dir}")

    def _read_csv_any(p: Path) -> pd.DataFrame:
        last_err = None
        for enc in ("utf-8-sig", "utf-8", "gb18030", "cp1252"):
            try:
                return pd.read_csv(p, encoding=enc)
            except Exception as e:
                last_err = e
        raise last_err

    for f in files:
        df = _read_csv_any(f)
        if "customerID" not in df.columns:
            report.append({"file": f.name, "rows_before": len(df), "rows_after": len(df),
                           "dropped": 0, "note": "no customerID col"})
            continue

        # æ ‡å‡†åŒ–ID
        ids = df["customerID"].astype(str).str.strip()
        dup_mask = ids.duplicated(keep=False)  # True è¡¨ç¤ºè¯¥IDåœ¨æœ¬æ–‡ä»¶å‡ºç°â‰¥2æ¬¡
        dropped = int(dup_mask.sum())

        if dropped > 0:
            df_clean = df.loc[~dup_mask].copy()
            # åŸåœ°è¦†ç›–å†™å›ï¼Œé¿å…Windowså¤šç©ºè¡Œ
            df_clean.to_csv(f, index=False, encoding="utf-8-sig", lineterminator="\n")
            report.append({"file": f.name, "rows_before": len(df), "rows_after": len(df_clean),
                           "dropped": dropped, "note": "duplicates removed"})
        else:
            # ä¹Ÿç»Ÿä¸€å†™ä¸€ä¸‹ï¼Œä¿è¯æ¢è¡Œæ ¼å¼ä¸€è‡´ï¼ˆå¯æ³¨é‡Šï¼‰
            df.to_csv(f, index=False, encoding="utf-8-sig", lineterminator="\n")
            report.append({"file": f.name, "rows_before": len(df), "rows_after": len(df),
                           "dropped": 0, "note": "no duplicates"})

    # return report
def drop_unknown_rows(df: pd.DataFrame) -> pd.DataFrame:
    """
    åˆ é™¤ä»»æ„åˆ—ç­‰å€¼ä¸ºä»¥ä¸‹è„å€¼çš„è¡Œï¼ˆå¤§å°å†™ä¸æ•æ„Ÿï¼›å‰åç©ºæ ¼/ä¸­åˆ’çº¿ç­‰å·²å½’ä¸€ï¼‰ï¼š
      unknown, unk, na, n/a, null, none, nan, -, â€”, ç©ºä¸²
    æ³¨æ„ï¼šæ˜¯â€œç­‰å€¼â€åŒ¹é…ï¼Œä¸ä¼šè¯¯ä¼¤ 'unknownable' ä¹‹ç±»ã€‚
    """
    if df.empty:
        return df

    # ç»Ÿä¸€åˆ°å°å†™+å»é¦–å°¾ç©ºç™½+æŠŠä¸­æ–‡/é•¿ç ´æŠ˜å·ç­‰å½’ä¸€æˆ '-'
    norm = df.apply(
        lambda s: (s.astype(str)
                     .str.strip()
                     .str.lower()
                     .str.replace("\u2014", "-", regex=False)  # â€” â†’ -
                     .str.replace("\u2013", "-", regex=False)  # â€“ â†’ -
                     .str.replace(r"\s*/\s*", "/", regex=True) # n / a â†’ n/a
                  )
    )

    bad_tokens = {"unknown", "unk", "na", "n/a", "null", "none", "nan", "-", ""}

    # ä»»æ„åˆ—å‘½ä¸­ä¸Šè¿°â€œç­‰å€¼â€è„å€¼å°±åˆ¤ä¸ºåè¡Œ
    bad_mask = norm.apply(lambda col: col.isin(bad_tokens))
    bad_any = bad_mask.any(axis=1)

    return df.loc[~bad_any].copy()



def main():


    # === åŠ è½½æ‰€æœ‰ç‰¹å¾è¡¨ ===
    log("åŠ è½½ Silver ç‰¹å¾è¡¨...")
    features = {}
    for name, path in feature_paths.items():
        df = safe_read_csv(path)
        if not df.empty:
            features[name] = df
            log(f"âœ… Loaded {name} ({len(df)} rows)")
        else:
            log(f"âš ï¸ Skip {name}")

    label_files = sorted(LABEL_DIR.glob("lable_*.csv"))
    if not label_files:
        log(f"âŒ æœªæ‰¾åˆ°ä»»ä½• lable_*.csv æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥ç›®å½•ï¼š{LABEL_DIR}")
        exit(1)

    for label_path in sorted(LABEL_DIR.glob("lable_*.csv")):
        label_name = label_path.stem.replace("lable_", "")
        log(f"\nğŸŸ¡ å¤„ç†æ ‡ç­¾æ–‡ä»¶ï¼š{label_path.name}")

        # è¯»å– & æ ‡å‡†åŒ–
        label_df = pd.read_csv(label_path)
        if "customerID" not in label_df.columns:
            log(f"âš ï¸ {label_path.name} ç¼ºå°‘ customerIDï¼Œè·³è¿‡ã€‚")
            continue
        label_df["customerID"] = label_df["customerID"].astype(str).str.strip()

        # â‘  å…ˆæ¸… label çš„ unknown è¡Œ
        merged = drop_unknown_rows(label_df)

        # â‘¡ é€ä¸ªç‰¹å¾è¡¨ï¼šå…ˆæ¸… unknownï¼Œå†æŒ‰ ID å»é‡ï¼Œæœ€ååˆå¹¶
        for name, fdf in features.items():
            if fdf.empty:
                continue
            f = fdf.copy()
            if "customerID" not in f.columns:
                log(f"âš ï¸ ç‰¹å¾è¡¨ {name} ç¼ºå°‘ customerIDï¼Œè·³è¿‡è¯¥è¡¨ã€‚")
                continue
            f["customerID"] = f["customerID"].astype(str).str.strip()

            # æ¸… unknown è¡Œ
            f = drop_unknown_rows(f)
            # é¿å…ä¸€å¯¹å¤šè†¨èƒ€ï¼šç›¸åŒ ID åªä¿ç•™æœ€åä¸€æ¡ï¼ˆæˆ–æ”¹ 'first'ï¼‰
            f = f.drop_duplicates(subset=["customerID"], keep="last")

            merged = merged.merge(f, on="customerID", how="inner")

        # â‘¢ åˆå¹¶å®Œæˆå†å…œåº•æ¸…ä¸€æ¬¡ unknownï¼Œå¹¶æŒ‰ ID å»é‡
        merged = drop_unknown_rows(merged)
        merged = merged.drop_duplicates(subset=["customerID"], keep="last")

        out_path = OUT_DIR / f"gold_{label_name}.csv"
        merged.to_csv(out_path, index=False, encoding="utf-8-sig", lineterminator="\n")
        log(f"âœ… è¾“å‡ºæ–‡ä»¶ï¼š{out_path}ï¼ˆ{len(merged)} è¡Œï¼‰")

    drop_all_duplicate_customer_ids_in_gold(BASE_DIR)

    log("\nğŸ‰ Gold å±‚ç‰¹å¾åˆå¹¶å®Œæˆï¼")

if __name__ == "__main__":
    main()


