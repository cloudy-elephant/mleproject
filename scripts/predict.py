import os
import argparse
import pandas as pd
import numpy as np
from tqdm import tqdm
import joblib
from datetime import datetime
from pathlib import Path
import os

# =========================================================
# âœ… predict.py
# ä» gold feature store è¯»å–ç‰¹å¾æ•°æ®
# åŠ è½½å·²è®­ç»ƒæ¨¡å‹ + scalerï¼Œç”Ÿæˆæ¯æœˆé¢„æµ‹ç»“æœ
# =========================================================

default_datamart = (
    Path(__file__).resolve().parents[1] / "datamart/gold/feature_store"
    if os.name == "nt"
    else Path("/opt/airflow/datamart")
)
FEATURE_DIR = Path(os.getenv("DATA_DIR", default_datamart))

PRED_DIR    = "./datamart/gold/predictions"

os.makedirs(PRED_DIR, exist_ok=True)

# ä¼˜å…ˆç¯å¢ƒå˜é‡ï¼Œå¦åˆ™å°è¯•æœ¬åœ°é¡¹ç›®ç»“æ„
default_model_path = (
    Path(__file__).resolve().parents[1] / "model_bank"
    if os.name == "nt"  # Windows
    else Path("/opt/airflow/model_bank")
)
MODEL_BANK_DIR = Path(os.getenv("MODEL_BANK_DIR", default_model_path))


def _mb(*parts) -> str:
    """æ‹¼å‡º model_bank ä¸‹çš„ç»å¯¹è·¯å¾„ï¼Œå¹¶åšå­˜åœ¨æ€§æ£€æŸ¥ç»™å‡ºå‹å¥½æŠ¥é”™ã€‚"""
    p = (MODEL_BANK_DIR.joinpath(*parts)).resolve()
    if not p.exists():
        raise FileNotFoundError(f"missing artifact: {p} "
                                f"(MODEL_BANK_DIR={MODEL_BANK_DIR.as_posix()})")
    return p.as_posix()


# =========================================================
# ğŸ”¹ å·¥å…·å‡½æ•°
# =========================================================
def norm_id(x):
    if pd.isna(x):
        return x
    return str(x).strip().upper()

def to_ymd_str(s):
    s = pd.to_datetime(s, errors="coerce")
    return s.dt.strftime("%Y-%m-%d")

# =========================================================
# ğŸ”¹ ä¸»å‡½æ•°
# =========================================================
def main(snapshotdate: str):
    print(f"\nğŸš€ Running model inference for {snapshotdate}")

    # === 1ï¸âƒ£ åŠ è½½æ¨¡å‹å’Œ scaler ===
    scaler_path = _mb("v1", "scaler.joblib")
    model_path = _mb("v1", "logreg_model.joblib")

    scaler = joblib.load(scaler_path)
    model  = joblib.load(model_path)

    print("âœ… æ¨¡å‹ä¸ScaleråŠ è½½å®Œæ¯•")

    # === 2ï¸âƒ£ è¯»å–ç‰¹å¾è¡¨ï¼ˆ3ä¸ªå­è¡¨ï¼‰===
    attr_path = os.path.join(FEATURE_DIR, "attributes_feature.parquet")
    clk_path  = os.path.join(FEATURE_DIR, "clickstream_features.parquet")
    fin_path  = os.path.join(FEATURE_DIR, "financial_feature.parquet")

    attributes = pd.read_parquet(attr_path)
    clickstream = pd.read_parquet(clk_path)
    financial  = pd.read_parquet(fin_path)

    # inner joinï¼ˆæŒ‰ Customer_ID + snapshot_dateï¼‰
    features = attributes.merge(clickstream, on=["Customer_ID", "snapshot_date"], how="inner")
    features = features.merge(financial,  on=["Customer_ID", "snapshot_date"], how="inner")

    features["Customer_ID"]   = features["Customer_ID"].map(norm_id)
    features["snapshot_date"] = to_ymd_str(features["snapshot_date"])

    print(f"âœ… ç‰¹å¾è¡¨åˆå¹¶å®Œæˆ: {features.shape}")

    # === 3ï¸âƒ£ è¿‡æ»¤å½“å‰æœˆä»½ ===
    features = features[features["snapshot_date"] == snapshotdate]
    if features.empty:
        raise ValueError(f"âš ï¸ æ²¡æœ‰æ‰¾åˆ° snapshot_date={snapshotdate} çš„ç‰¹å¾æ•°æ®")

    print(f"ğŸ“… æœ¬æ¬¡é¢„æµ‹æ ·æœ¬æ•°: {len(features)}")

    # === 4ï¸âƒ£ æ¨¡å‹æ¨ç† ===
    X = features.drop(columns=["Customer_ID", "snapshot_date"], errors="ignore")

    # ç¼ºå¤±å€¼å¤„ç†ï¼šå…ˆå¡«0
    X = X.fillna(0)

    # ç‰¹å¾å¯¹é½ï¼šé˜²æ­¢ç¼ºå°‘è®­ç»ƒæ—¶çš„åˆ—
    if hasattr(scaler, "feature_names_in_"):
        missing_cols = [c for c in scaler.feature_names_in_ if c not in X.columns]
        if missing_cols:
            X = pd.concat([X, pd.DataFrame(0, index=X.index, columns=missing_cols)], axis=1)
        X = X[scaler.feature_names_in_]
        print(f"ğŸ§© å·²è¡¥é½ç¼ºå¤±ç‰¹å¾: {len(missing_cols)} åˆ—")

    Xs = scaler.transform(X)
    probs = model.predict_proba(Xs)[:, 1]
    preds = (probs > 0.5).astype(int)

    # === 5ï¸âƒ£ ä¿å­˜é¢„æµ‹ç»“æœ ===
    out_df = features[["Customer_ID", "snapshot_date"]].copy()
    out_df["churn_prob"] = probs
    out_df["churn_pred"] = preds

    out_path = os.path.join(PRED_DIR, f"gold_pred_{snapshotdate.replace('-', '_')}.parquet")
    out_df.to_parquet(out_path, index=False)

    print(f"âœ… é¢„æµ‹å®Œæˆå¹¶ä¿å­˜åˆ°: {out_path}")
    print(f"ğŸ”¢ æ­£æ ·æœ¬é¢„æµ‹ç‡: {out_df['churn_pred'].mean():.4f}")

# =========================================================
# ğŸ§­ å‘½ä»¤è¡Œæ¥å£
# =========================================================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run monthly churn prediction")
    parser.add_argument("--snapshotdate", type=str, required=True, help="é¢„æµ‹æœˆä»½ (YYYY-MM-DD æˆ– YYYY_MM_DD)")
    args = parser.parse_args()

    # ä¿®æ­£æ ¼å¼
    snapshotdate = args.snapshotdate.replace("_", "-")
    try:
        datetime.strptime(snapshotdate, "%Y-%m-%d")
    except ValueError:
        raise ValueError("âŒ snapshotdate æ ¼å¼åº”ä¸º YYYY-MM-DD æˆ– YYYY_MM_DD")

    main(snapshotdate)
