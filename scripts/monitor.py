# monitor.py
import os, glob, re
import numpy as np
import pandas as pd
import joblib
from tqdm import tqdm
from sklearn.metrics import (
    roc_auc_score, f1_score, precision_score, recall_score,
    average_precision_score, brier_score_loss
)
import matplotlib.pyplot as plt
from pathlib import Path

# ========= é…ç½®åŒº =========
# default_model_path = (
#     Path(__file__).resolve().parents[1] / "model_bank"
#     if os.name == "nt"  # Windows
#     else Path("/opt/airflow/model_bank")
# )
# MODEL_BANK_DIR = Path(os.getenv("MODEL_BANK_DIR", default_model_path))
from pathlib import Path
import os

is_windows = os.name == "nt"

# é¡¹ç›®æ ¹ï¼šæœ¬åœ°=ä»“åº“æ ¹ï¼›å®¹å™¨=/opt/airflow
BASE_DIR = Path(__file__).resolve().parents[1] if is_windows else Path("/opt/airflow")

# æ¨¡å‹æ ¹ç›®å½•ï¼ˆå¯ç”¨ç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
MODEL_BANK_DIR = Path(os.getenv("MODEL_BANK_DIR", BASE_DIR / "model_bank"))

# ç‰ˆæœ¬å·ï¼ˆé»˜è®¤ v1ï¼Œå¯ç”¨ç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
MODEL_VERSION = os.getenv("MODEL_VERSION", "v1")

# datamart æ ¹ç›®å½•ï¼ˆå¯ç”¨ç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
DATAMART_DIR = Path(os.getenv("DATA_DIR", BASE_DIR / "datamart"))

# å­ç›®å½•
LABEL_DIR   = DATAMART_DIR / "gold" / "label_store"
FEATURE_DIR = DATAMART_DIR / "gold" / "feature_store"
PRED_DIR    = DATAMART_DIR / "gold" / "predictions"
OUT_DIR     = DATAMART_DIR / "gold" / "monitoring"

def _mb(*parts) -> Path:
    """
    åœ¨ model_bank/<MODEL_VERSION>/ ä¸‹æ‹¼è·¯å¾„ï¼›è‹¥ä¸å­˜åœ¨ï¼Œå†å›é€€åˆ° model_bank æ ¹ä¸‹æ‰¾ã€‚
    """
    # é¦–é€‰ï¼šå¸¦ç‰ˆæœ¬ç›®å½•
    p1 = (MODEL_BANK_DIR / MODEL_VERSION).joinpath(*parts)
    if p1.exists():
        return p1.resolve()
    # é€€è·¯ï¼šä¸å¸¦ç‰ˆæœ¬ï¼ˆå…¼å®¹è€æ–‡ä»¶ï¼‰
    p2 = MODEL_BANK_DIR.joinpath(*parts)
    if p2.exists():
        return p2.resolve()
    raise FileNotFoundError(
        f"missing artifact; tried: {p1.as_posix()} and {p2.as_posix()} "
        f"(MODEL_BANK_DIR={MODEL_BANK_DIR.as_posix()}, MODEL_VERSION={MODEL_VERSION})"
    )


THRESHOLD = 0.5

os.makedirs(PRED_DIR, exist_ok=True)
os.makedirs(OUT_DIR, exist_ok=True)
# ====================================


# ---------- å‡½æ•°å®šä¹‰ ----------

def merge_features_and_labels(
    feature_dir: str,
    label_dir: str,
    start_date: str = "2024_07_01",
    end_date: str = "2024_12_01"
) -> tuple[pd.DataFrame, pd.DataFrame]:
    """åˆå¹¶ feature_store ä¸‹ä¸‰å¼ è¡¨ä¸ label_store ä¸‹å„æœˆæ ‡ç­¾ï¼ˆinner join by Customer_IDï¼‰"""

    def norm_id(x):
        if pd.isna(x):
            return x
        return str(x).strip().upper()

    def to_ymd_str(s):
        s = pd.to_datetime(s, errors="coerce")
        return s.dt.strftime("%Y-%m-%d")

    # === è¯»å–ä¸‰å¼ ç‰¹å¾è¡¨ ===
    attr_path = os.path.join(feature_dir, "attributes_feature.parquet")
    clk_path  = os.path.join(feature_dir, "clickstream_features.parquet")
    fin_path  = os.path.join(feature_dir, "financial_feature.parquet")

    attributes  = pd.read_parquet(attr_path)
    clickstream = pd.read_parquet(clk_path)
    financial   = pd.read_parquet(fin_path)

    features = attributes.merge(clickstream, on=["Customer_ID", "snapshot_date"], how="inner")
    features = features.merge(financial,  on=["Customer_ID", "snapshot_date"], how="inner")
    print(f"âœ… features merge å®Œæˆ: {features.shape}")

    features["Customer_ID"]   = features["Customer_ID"].map(norm_id)
    features["snapshot_date"] = to_ymd_str(features["snapshot_date"])

    # === éå† label æ–‡ä»¶ ===
    label_files = sorted(glob.glob(os.path.join(label_dir, "gold_label_store_*.parquet")))
    summary_rows, merged_list = [], []
    # start_dt, end_dt = pd.to_datetime(start_date), pd.to_datetime(end_date)
    start_dt = pd.to_datetime(start_date.replace("_", "-"))
    end_dt = pd.to_datetime(end_date.replace("_", "-"))

    for fpath in label_files:
        base = os.path.basename(fpath)
        print(f"\nğŸ“„ æ­£åœ¨å¤„ç†: {base}")
        m = re.search(r"(\d{4})_(\d{2})_(\d{2})", base)
        if not m:
            print("âš ï¸ æ–‡ä»¶åæ— æ—¥æœŸï¼Œè·³è¿‡")
            continue
        snapshot_date = pd.to_datetime(f"{m.group(1)}-{m.group(2)}-{m.group(3)}")
        if not (start_dt <= snapshot_date <= end_dt):
            print(f"â­ï¸ è·³è¿‡ {snapshot_date.date()}ï¼ˆä¸åœ¨æ—¶é—´çª—å†…ï¼‰")
            continue

        labels = pd.read_parquet(fpath)
        if "Customer_ID" not in labels.columns:
            print("âš ï¸ ç¼ºå°‘ Customer_IDï¼Œè·³è¿‡")
            continue

        labels["Customer_ID"] = labels["Customer_ID"].map(norm_id)
        labels["snapshot_date"] = snapshot_date.strftime("%Y-%m-%d")

        merged = features.merge(labels, on="Customer_ID", how="inner")
        print(f"âœ… merge åè¡Œæ•°: {merged.shape[0]}, åˆ—æ•°: {merged.shape[1]}")

        merged_list.append(merged)
        summary_rows.append({
            "file": base,
            "snapshot_date": snapshot_date.strftime("%Y-%m-%d"),
            "merged_rows": merged.shape[0],
            "merged_cols": merged.shape[1],
        })

    if not merged_list:
        raise RuntimeError("âŒ æ²¡æœ‰ä»»ä½• label æ–‡ä»¶æˆåŠŸ mergeï¼Œè¯·æ£€æŸ¥è·¯å¾„æˆ–æ—¶é—´çª—å£ã€‚")

    merged_all_df = pd.concat(merged_list, ignore_index=True)
    summary_df = pd.DataFrame(summary_rows)
    print("\nğŸ“Š å„æ–‡ä»¶ merge åçš„ç»“æœï¼š")
    print(summary_df)
    return merged_all_df, summary_df


def compute_monitor_metrics(
    merged_df: pd.DataFrame,
    prob_col: str = "churn_prob",
    label_col: str = "label",
    month_col: str = "snapshot_date"
) -> pd.DataFrame:
    """è®¡ç®—æ¯æœˆç›‘æ§æŒ‡æ ‡ï¼ˆAUCã€F1ã€KSã€PSIç­‰ï¼‰"""

    def ks_stat(y_true, y_prob):
        df = pd.DataFrame({"y": y_true, "p": y_prob}).sort_values("p")
        pos = (df["y"] == 1).cumsum() / max((df["y"] == 1).sum(), 1)
        neg = (df["y"] == 0).cumsum() / max((df["y"] == 0).sum(), 1)
        return np.max(np.abs(pos - neg))

    def psi(actual, expected, bins=10, eps=1e-6):
        cuts = np.quantile(expected, np.linspace(0, 1, bins+1))
        cuts[0], cuts[-1] = -np.inf, np.inf
        e_cnt = np.histogram(expected, bins=cuts)[0] / (len(expected)+eps)
        a_cnt = np.histogram(actual,   bins=cuts)[0] / (len(actual)+eps)
        return np.sum((a_cnt - e_cnt) * np.log((a_cnt + eps) / (e_cnt + eps)))

    merged_df = merged_df.copy()
    merged_df[month_col] = pd.to_datetime(merged_df[month_col], errors="coerce")
    merged_df["month_str"] = merged_df[month_col].dt.to_period("M").astype(str)

    metrics, base_prob = [], None
    for month, dfm in merged_df.groupby("month_str"):
        y = dfm[label_col].values
        p = dfm[prob_col].values
        yhat = (p > THRESHOLD).astype(int)
        if len(np.unique(y)) < 2:
            print(f"âš ï¸ {month} æ ‡ç­¾å…¨ä¸ºåŒç±»ï¼Œè·³è¿‡æŒ‡æ ‡è®¡ç®—ã€‚")
            continue
        m = {
            "month": month,
            "n": len(dfm),
            "pos": int(y.sum()),
            "auc": roc_auc_score(y, p),
            "pr_auc": average_precision_score(y, p),
            "f1": f1_score(y, yhat),
            "precision": precision_score(y, yhat),
            "recall": recall_score(y, yhat),
            "ks": ks_stat(y, p),
            "brier": brier_score_loss(y, p),
            "pd_rate": p.mean(),
        }
        if base_prob is None:
            m["psi_vs_base"] = np.nan
            base_prob = p
        else:
            m["psi_vs_base"] = psi(p, base_prob)
        metrics.append(m)

    result = pd.DataFrame(metrics)
    print("âœ… å·²è®¡ç®—å„æœˆç›‘æ§æŒ‡æ ‡ï¼š")
    print(result)
    return result


def load_model_and_scaler(model_dir: str):
    # scaler_path = os.path.join(model_dir, "scaler.joblib")
    scaler_path = _mb("scaler.joblib")
    model_path = _mb("logreg_model.joblib")  # æŒ‰ä½ çš„å®é™…æ¨¡å‹æ–‡ä»¶å
    # model_path  = os.path.join(model_dir, "logreg_model.joblib")
    scaler = joblib.load(scaler_path)
    model  = joblib.load(model_path)
    return scaler, model


def plot_monitor_trends(mdf: pd.DataFrame, out_dir: str = OUT_DIR):
    os.makedirs(out_dir, exist_ok=True)
    for col in ["auc", "pr_auc", "pd_rate", "psi_vs_base"]:
        plt.figure(figsize=(6,4))
        plt.plot(mdf["month"], mdf[col], marker="o")
        plt.title(f"{col.upper()} Trend")
        plt.xlabel("Month")
        plt.ylabel(col)
        plt.xticks(rotation=45)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(os.path.join(out_dir, f"{col}_trend.png"), dpi=150)
        plt.close()
    print(f"ğŸ“ˆ è¶‹åŠ¿å›¾å·²ä¿å­˜åˆ° {out_dir}")

def credit_history_to_months(text):
    """
    æŠŠ '19 Years and 9 Months' è½¬æˆæ€»æœˆä»½æ•°ï¼ˆintï¼‰ã€‚
    """
    if pd.isna(text):
        return np.nan

    text = str(text)
    # åŒ¹é…ä¸¤ä¸ªæ•°å­—ï¼ˆä¾‹å¦‚ 19 å’Œ 9ï¼‰
    match = re.findall(r'(\d+)', text)
    if len(match) >= 2:
        years = int(match[0])
        months = int(match[1])
    elif len(match) == 1:
        years = int(match[0])
        months = 0
    else:
        return np.nan

    return years * 12 + months

def drop_useless_columns_and_onehot_coding(df: pd.DataFrame) -> pd.DataFrame:
    id_cols = ["Customer_ID", "Credit_History_Age", "Name", "SSN",
               "snapshot_date_x", "snapshot_date_y", "label_def", "loan_id"]

    cat_cols = [
        c for c in df.columns
        if df[c].dtype == "object" and c not in id_cols
    ]

    # åˆ é™¤æ— å…³åˆ—
    df = df.drop(columns=["Name", "SSN", "snapshot_date_x", "label_def", "loan_id"], errors="ignore")

    # âœ… ä¿ç•™ label çš„ snapshot_dateï¼ˆ_yï¼‰
    if "snapshot_date_y" in df.columns:
        df = df.rename(columns={"snapshot_date_y": "snapshot_date"})
    else:
        # å…œåº•ï¼šå¦‚æœæ²¡æœ‰ snapshot_date_yï¼Œä¿ç•™ x
        if "snapshot_date_x" in df.columns:
            df = df.rename(columns={"snapshot_date_x": "snapshot_date"})
        else:
            df["snapshot_date"] = np.nan

    # One-hot ç¼–ç 
    df = pd.get_dummies(df, columns=cat_cols, drop_first=True)

    return df





# ---------- ä¸»æµç¨‹ ----------
def main():
    # Step 1. åŠ è½½æ¨¡å‹
    scaler, model = load_model_and_scaler(MODEL_BANK_DIR)
    print("âœ… æ¨¡å‹ä¸ScaleråŠ è½½å®Œæ¯•")

    # Step 2. åˆå¹¶ç‰¹å¾ä¸æ ‡ç­¾
    merged_all_df, summary_df = merge_features_and_labels(FEATURE_DIR, LABEL_DIR)

    # print('---------------------')
    # print(merged_all_df.columns)

    merged_all_df["Credit_History_Age_months"] = merged_all_df["Credit_History_Age"].apply(credit_history_to_months)
    merged_all_df = merged_all_df.drop(columns=["Credit_History_Age"])

    merged_all_df = drop_useless_columns_and_onehot_coding(merged_all_df)
    # print('---------------------')
    # print(merged_all_df.shape)

    # Step 3. æ¨¡å‹æ¨ç†
    # ====== å’Œè®­ç»ƒæ—¶åˆ—åå¯¹é½ï¼ˆå…³é”®è¡¥ä¸ï¼‰======
    # 1) è®­ç»ƒæ—¶çš„åˆ—é¡ºåºï¼ˆscikit-learn 1.0+ ä¼šå¸¦è¿™ä¸ªå±æ€§ï¼‰
    if not hasattr(scaler, "feature_names_in_"):
        raise RuntimeError(
            "å½“å‰ scaler æ²¡æœ‰ feature_names_in_ å±æ€§ã€‚å»ºè®®åœ¨è®­ç»ƒæ—¶ç”¨ DataFrame æ‹Ÿåˆï¼Œ"
            "æˆ–æŠŠè®­ç»ƒæ—¶çš„ç‰¹å¾åˆ—ä¿å­˜ä¸º feature_list.json å¹¶åœ¨æ­¤è¯»å–ã€‚"
        )

    expected_cols = list(scaler.feature_names_in_)

    # 2) é¢„æµ‹ç”¨çš„ç‰¹å¾ï¼ˆå…ˆæŠŠä¸éœ€è¦çš„å‰”æ‰ï¼‰
    # ä½ çš„ä»£ç åŸæ¥æ˜¯ï¼šfeature_cols = [c for c in merged_all_df.columns if c not in ["Customer_ID", "label"]]
    # è¿™é‡Œæ”¹ä¸ºï¼šä»¥å®é™… one-hot åçš„æ‰€æœ‰åˆ—ä¸ºå€™é€‰ï¼Œç„¶åæŒ‰ expected å¯¹é½
    X = merged_all_df.drop(
        columns=[c for c in ["Customer_ID", "label", "snapshot_date"] if c in merged_all_df.columns],
        errors="ignore"
    )

    # å»æ‰é‡å¤åˆ—ï¼ˆæœ‰æ—¶ get_dummies ä¼šç”Ÿæˆé‡å¤åï¼‰
    X = X.loc[:, ~X.columns.duplicated()]

    # 3) ç¼ºå¤±çš„è®­ç»ƒåˆ—è¡¥ 0ï¼›å¤šä½™çš„æ¨ç†åˆ—åˆ é™¤
    missing = [c for c in expected_cols if c not in X.columns]
    extra = [c for c in X.columns if c not in expected_cols]

    if missing:
        # ä¸€æ¬¡æ€§åˆ›å»ºæ‰€æœ‰ç¼ºå¤±åˆ—å¹¶æ‹¼æ¥
        missing_df = pd.DataFrame(
            {c: np.zeros(len(X), dtype=float) for c in missing},
            index=X.index
        )
        X = pd.concat([X, missing_df], axis=1)

    if extra:
        # ä¸¢æ‰è®­ç»ƒæ—¶æ²¡è§è¿‡çš„åˆ—ï¼ˆé¿å… transform æŠ¥é”™ï¼‰
        X = X.drop(columns=extra)

    # 4) ä¸¥æ ¼æŒ‰è®­ç»ƒæ—¶é¡ºåºé‡æ’ï¼›å¹¶ç¡®ä¿æ•°å€¼ç±»å‹
    X = X[expected_cols].astype(float)

    X = X.fillna(0)
    Xs = scaler.transform(X)
    prob = model.predict_proba(Xs)[:, 1]
    merged_all_df["churn_prob"] = prob
    merged_all_df["churn_pred"] = (prob > THRESHOLD).astype("int8")
    print("âœ… å·²å®Œæˆé¢„æµ‹å¹¶å†™å…¥ merged_all_df")

    # Step 4. è®¡ç®—ç›‘æ§æŒ‡æ ‡
    monitor_df = compute_monitor_metrics(merged_all_df)
    monitor_df.to_csv(os.path.join(OUT_DIR, "monitor_summary.csv"), index=False)
    print(f"âœ… ç›‘æ§ç»“æœå·²ä¿å­˜åˆ° {OUT_DIR}/monitor_summary.csv")

    # Step 5. ç»˜åˆ¶è¶‹åŠ¿å›¾
    plot_monitor_trends(monitor_df, OUT_DIR)







if __name__ == "__main__":
    main()
