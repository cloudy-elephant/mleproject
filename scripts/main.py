# import os
# import glob
# import pandas as pd
# import matplotlib.pyplot as plt
# import numpy as np
# import random
# from datetime import datetime, timedelta
# from dateutil.relativedelta import relativedelta
# import pprint
# import pyspark
# import pyspark.sql.functions as F
# from pyspark.sql import SparkSession
#
# from pyspark.sql.functions import col
# from pyspark.sql.types import StringType, IntegerType, FloatType, DateType
#
# from utils.Bronze_Process import process_bronze_table
# from utils.Silver_Process import process_silver_table
# from utils.Silver_Process import process_silver_table_cs
# from utils.Silver_Process import process_silver_table_attributes
# from utils.Silver_Process import process_silver_table_financial
# from utils.Gold_Process import process_labels_gold_table
#
#
# # Initialize SparkSession
# spark = pyspark.sql.SparkSession.builder \
#     .appName("dev") \
#     .master("local[*]") \
#     .getOrCreate()
#
# # Set log level to ERROR to hide warnings
# spark.sparkContext.setLogLevel("ERROR")
#
# # set up config
# snapshot_date_str = "2023-01-01"
#
# start_date_str = "2023-01-01"
# end_date_str = "2024-12-01"
#
#
# # generate list of dates to process
# def generate_first_of_month_dates(start_date_str, end_date_str):
#     # Convert the date strings to datetime objects
#     start_date = datetime.strptime(start_date_str, "%Y-%m-%d")
#     end_date = datetime.strptime(end_date_str, "%Y-%m-%d")
#
#     # List to store the first of month dates
#     first_of_month_dates = []
#
#     # Start from the first of the month of the start_date
#     current_date = datetime(start_date.year, start_date.month, 1)
#
#     while current_date <= end_date:
#         # Append the date in yyyy-mm-dd format
#         first_of_month_dates.append(current_date.strftime("%Y-%m-%d"))
#
#         # Move to the first of the next month
#         if current_date.month == 12:
#             current_date = datetime(current_date.year + 1, 1, 1)
#         else:
#             current_date = datetime(current_date.year, current_date.month + 1, 1)
#
#     return first_of_month_dates
#
#
# dates_str_lst = generate_first_of_month_dates(start_date_str, end_date_str)
# print(dates_str_lst)
#
# # create bronze datalake
# bronze_lms_directory = "datamart/bronze/lms/"
#
# if not os.path.exists(bronze_lms_directory):
#     os.makedirs(bronze_lms_directory)
#
# # run bronze backfill
# for date_str in dates_str_lst:
#     process_bronze_table(date_str, bronze_lms_directory, spark)
#
# # ------------------------Silver--------------------------------------------------
#
# # silver: clean loan daily table
# silver_loan_daily_directory = "datamart/silver/loan_daily/"
#
# if not os.path.exists(silver_loan_daily_directory):
#     os.makedirs(silver_loan_daily_directory)
#
# for date_str in dates_str_lst:
#     process_silver_table(date_str, bronze_lms_directory, silver_loan_daily_directory, spark)
#
# # silver: clean financial table
# silver_loan_daily_directory_financial = "datamart/silver/financial/"
# process_silver_table_financial(silver_loan_daily_directory_financial, spark)
#
# # silver: clean attributed table
# silver_loan_daily_directory_attributes = "datamart/silver/attributes/"
# process_silver_table_attributes(silver_loan_daily_directory_attributes, spark)
#
# # silver: clean clickstream table
# silver_loan_daily_directory_cs = "datamart/silver/click_stream/"
# process_silver_table_cs(silver_loan_daily_directory_cs, spark)
#
# # ------------------------Gold--------------------------------------------------
# silver_loan_daily_directory_financial = "datamart/gold/feature_store/"
# process_silver_table_financial(silver_loan_daily_directory_financial, spark)
#
# silver_loan_daily_directory_attributes = "datamart/gold/feature_store/"
# process_silver_table_attributes(silver_loan_daily_directory_attributes, spark)
#
# silver_loan_daily_directory_cs = "datamart/gold/feature_store/"
# process_silver_table_cs(silver_loan_daily_directory_cs, spark)
#
# gold_label_store_directory = "/datamart/gold/label_store/"
# if not os.path.exists(gold_label_store_directory):
#     os.makedirs(gold_label_store_directory)
#
# for date_str in dates_str_lst:
#     process_labels_gold_table(date_str, silver_loan_daily_directory, gold_label_store_directory, spark, dpd = 30, mob = 6)
#


# scripts/main.py
import os
import argparse
from datetime import datetime, timedelta
from typing import List

import pyspark
from pyspark.sql import SparkSession

# --- æœ¬åœ°å¯¼å…¥ ---
# è¿™äº›å‡½æ•°åœ¨ä½ çš„ utils é‡Œï¼Œä¿æŒä½ ç°æœ‰çš„ç­¾åï¼š
#   process_bronze_table(date_str, bronze_lms_directory, spark)
#   process_silver_table(date_str, bronze_lms_directory, silver_loan_daily_directory, spark)
#   process_silver_table_financial(output_dir, spark)
#   process_silver_table_attributes(output_dir, spark)
#   process_silver_table_cs(output_dir, spark)
#   process_labels_gold_table(date_str, silver_loan_daily_directory, gold_label_store_directory, spark, dpd=30, mob=6)
from utils.Bronze_Process import process_bronze_table
from utils.Silver_Process import (
    process_silver_table,
    process_silver_table_financial,
    process_silver_table_attributes,
    process_silver_table_cs,
)
from utils.Gold_Process import process_labels_gold_table
from pathlib import Path
import os

# ========= é…ç½®ï¼ˆå¯æŒ‰éœ€ä¿®æ”¹é»˜è®¤å€¼ï¼‰ =========
DEFAULT_START = "2023-01-01"
DEFAULT_END   = "2024-12-01"

# é¡¹ç›®æ ¹ç›®å½• = scripts çš„ä¸Šä¸€çº§
PROJECT_ROOT = Path(__file__).resolve().parents[1]

# å…è®¸é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–æ ¹è·¯å¾„ï¼ˆä¾‹å¦‚ Airflow/Docker æ—¶å¯æ˜ å°„åˆ° /opt/airflow/datamartï¼‰
DATAMART_ROOT = Path(os.getenv("DATAMART_DIR", PROJECT_ROOT / "datamart"))

# ä¸‹é¢è¿™äº›æ›¿æ¢ä½ ç°åœ¨çš„å¸¸é‡
BRONZE_LMS_DIR         = str(DATAMART_ROOT / "bronze" / "lms")
SILVER_LOAN_DAILY_DIR  = str(DATAMART_ROOT / "silver" / "loan_daily")
SILVER_FINANCIAL_DIR   = str(DATAMART_ROOT / "silver" / "financial")
SILVER_ATTRIBUTES_DIR  = str(DATAMART_ROOT / "silver" / "attributes")
SILVER_CLICKSTREAM_DIR = str(DATAMART_ROOT / "silver" / "click_stream")

GOLD_FEATURE_DIR       = str(DATAMART_ROOT / "gold" / "feature_store")
GOLD_LABEL_STORE_DIR   = str(DATAMART_ROOT / "gold" / "label_store")

# ========================================


def ensure_dirs(*dirs: str) -> None:
    for d in dirs:
        os.makedirs(d, exist_ok=True)


def generate_first_of_month_dates(start_date_str: str, end_date_str: str) -> List[str]:
    start_date = datetime.strptime(start_date_str, "%Y-%m-%d")
    end_date   = datetime.strptime(end_date_str,   "%Y-%m-%d")
    dates = []
    current = datetime(start_date.year, start_date.month, 1)
    while current <= end_date:
        dates.append(current.strftime("%Y-%m-%d"))
        # è·³åˆ°ä¸‹æœˆ 1 å·
        next_month = current.replace(day=28) + timedelta(days=4)
        current = next_month.replace(day=1)
    return dates


def run_bronze(dates: List[str], spark: SparkSession) -> None:
    print(f"ğŸŸ¤ [Bronze] è¾“å‡ºç›®å½•: {BRONZE_LMS_DIR}")
    ensure_dirs(BRONZE_LMS_DIR)
    for ds in dates:
        print(f"  â†’ Bronze backfill @ {ds}")
        process_bronze_table(ds, BRONZE_LMS_DIR, spark)


def run_silver(dates: List[str], spark: SparkSession) -> None:
    print("âšª [Silver] æ¸…æ´—å¹¶ç”Ÿæˆä¸­é—´å±‚")
    ensure_dirs(SILVER_LOAN_DAILY_DIR, SILVER_FINANCIAL_DIR, SILVER_ATTRIBUTES_DIR, SILVER_CLICKSTREAM_DIR)

    # 1) é€æœˆç”± Bronze â†’ Silver loan_daily
    for ds in dates:
        print(f"  â†’ Silver loan_daily @ {ds}")
        # æ³¨æ„ï¼šæŒ‰ç…§ä½ ç°æœ‰ç­¾åï¼šprocess_silver_table(date_str, bronze_dir, silver_dir, spark)
        process_silver_table(ds, BRONZE_LMS_DIR, SILVER_LOAN_DAILY_DIR, spark)

    # 2) å…¶å®ƒä¸‰å¼  Silver è¡¨ï¼ˆè¿™äº›å‡½æ•°ä½¿ç”¨å†…éƒ¨è¯»å–/èšåˆï¼‰
    print("  â†’ Silver financial")
    process_silver_table_financial(SILVER_FINANCIAL_DIR, spark)

    print("  â†’ Silver attributes")
    process_silver_table_attributes(SILVER_ATTRIBUTES_DIR, spark)

    print("  â†’ Silver clickstream")
    process_silver_table_cs(SILVER_CLICKSTREAM_DIR, spark)


def run_gold(dates: List[str], spark: SparkSession, dpd: int = 30, mob: int = 6) -> None:
    print("ğŸŸ¡ [Gold] ç”Ÿæˆç‰¹å¾ & æ ‡ç­¾")
    # ä½ çš„ Silver â†’ Gold è¿‡ç¨‹ï¼šæ ¹æ®ä½ ä¹‹å‰çš„è„šæœ¬ï¼Œè¿™ä¸‰ä¸ªå‡½æ•°ä¹Ÿå¯ä»¥ç›´æ¥æŠŠäº§ç‰©å†™åˆ° gold/feature_store
    ensure_dirs(GOLD_FEATURE_DIR, GOLD_LABEL_STORE_DIR)

    # å°†ä¸‰ç±»ç‰¹å¾äº§å‡ºåˆ° GOLD_FEATURE_DIR
    print("  â†’ Gold feature_store (financial/attributes/clickstream)")
    process_silver_table_financial(GOLD_FEATURE_DIR, spark)
    process_silver_table_attributes(GOLD_FEATURE_DIR, spark)
    process_silver_table_cs(GOLD_FEATURE_DIR, spark)

    # æ ‡ç­¾è¡¨æŒ‰æœˆç”Ÿæˆï¼ˆä¾èµ– SILVER_LOAN_DAILY_DIRï¼‰
    for ds in dates:
        print(f"  â†’ Gold label_store @ {ds} (dpd={dpd}, mob={mob})")
        process_labels_gold_table(
            ds,
            SILVER_LOAN_DAILY_DIR,
            GOLD_LABEL_STORE_DIR,
            spark,
            dpd=dpd,
            mob=mob,
        )


def build_argparser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser(
        description="MLE Assignment Pipeline (Bronze/Silver/Gold)",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )
    p.add_argument("--start", default=DEFAULT_START, help="èµ·å§‹æœˆä»½ï¼ˆYYYY-MM-DDï¼Œä½¿ç”¨æ¯æœˆ1å·ï¼‰")
    p.add_argument("--end",   default=DEFAULT_END,   help="ç»“æŸæœˆä»½ï¼ˆYYYY-MM-DDï¼Œä½¿ç”¨æ¯æœˆ1å·ï¼‰")
    p.add_argument(
        "--stages",
        default="all",
        choices=["all", "bronze", "silver", "gold", "bronze_silver", "silver_gold"],
        help="é€‰æ‹©è¦è¿è¡Œçš„é˜¶æ®µ"
    )
    p.add_argument("--dpd", type=int, default=30, help="æ ‡ç­¾æ„é€ å‚æ•°ï¼šdays past due")
    p.add_argument("--mob", type=int, default=6,  help="æ ‡ç­¾æ„é€ å‚æ•°ï¼šmonths on book")
    return p


def main():
    args = build_argparser().parse_args()

    dates = generate_first_of_month_dates(args.start, args.end)
    print(f"ğŸ“… å°†å¤„ç†è¿™äº›æœˆä»½ï¼š{dates[0]} .. {dates[-1]} ï¼ˆå…± {len(dates)} ä¸ªæœˆï¼‰")

    # åˆå§‹åŒ– Spark
    spark = (
        pyspark.sql.SparkSession.builder
        .appName("dev")
        .master("local[*]")
        .getOrCreate()
    )
    spark.sparkContext.setLogLevel("ERROR")

    # ç¡®ä¿åŸºç¡€ç›®å½•å­˜åœ¨
    ensure_dirs(
        BRONZE_LMS_DIR,
        SILVER_LOAN_DAILY_DIR, SILVER_FINANCIAL_DIR, SILVER_ATTRIBUTES_DIR, SILVER_CLICKSTREAM_DIR,
        GOLD_FEATURE_DIR, GOLD_LABEL_STORE_DIR
    )

    stages = args.stages
    if stages in ("all", "bronze", "bronze_silver"):
        run_bronze(dates, spark)

    if stages in ("all", "silver", "bronze_silver", "silver_gold"):
        run_silver(dates, spark)

    if stages in ("all", "gold", "silver_gold"):
        run_gold(dates, spark, dpd=args.dpd, mob=args.mob)

    print("âœ… å…¨éƒ¨å®Œæˆã€‚")


if __name__ == "__main__":
    main()
